# -*- coding: utf-8 -*-
""" ã€Špythonæ•°æ®ç§‘å­¦ã€‹Sckit-Learnéƒ¨åˆ†
å·²è¿‡æ—¶ï¼Œå‚è€ƒScikit-Lean
Created on Thu Mar 20 23:29:12 2025

@author: 63517
"""

# ## 5.2ã€€Scikit-Learnç®€ä»‹
# æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•åº“ï¼Œç‰¹ç‚¹ ç®¡é“å¼å‘½ä»¤api
#

# ### 5.2.1 æ•°æ®è¡¨ç¤º
# - æ•°æ®è¡¨ç¤ºä¸ºDataFrame

# 1. æ•°æ®è¡¨ï¼š
#    æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œåˆ—è¡¨ç¤ºç‰¹å¾

# In[1]:


from sklearn.mixture import GaussianMixture
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.datasets import make_circles
from ipywidgets import interact, fixed
from sklearn.svm import SVC  # "Support vector classifier"
from sklearn.datasets import make_blobs
from scipy import stats
from numpy import nan
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import learning_curve
import seaborn
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.manifold import Isomap
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA  # 1.é€‰æ‹©æ¨¡å‹ç±»
from sklearn.naive_bayes import GaussianNB  # 1.é€‰æ‹©æ¨¡å‹ç±»
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from sklearn.datasets import fetch_lfw_people
from sklearn.metrics import precision_score, recall_score, f1_score
import jieba
from sklearn.model_selection import train_test_split  # åˆ†å‰²æ•°æ®é›†
import pandas as pd
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups  # 2000ç¯‡æ–°é—»
from scipy.stats import multivariate_normal
from sklearn.datasets import make_blobs  # åˆ›å»ºå‡ ç±»éšæœºæ•°æ®
from sklearn.datasets import fetch_20newsgroups
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import validation_curve
from sklearn.model_selection import train_test_split
from sklearn.mixture import GaussianMixture  # 1.é€‰æ‹©æ¨¡å‹ç±»
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split  # è‡ªåŠ¨åˆ†å‰²æ•°æ®é›†
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
iris = sns.load_dataset('iris')
iris.describe()


# 2. ç‰¹å¾çŸ©é˜µï¼šæ¯è¡Œæ˜¯å¯¹è±¡ï¼Œæ¯åˆ—éƒ½æ˜¯é‡åŒ–å€¼ã€‚[n_samples, n_features]
# 3. ç›®æ ‡ï¼ˆæ ‡ç­¾ï¼‰æ•°ç»„ï¼š[n_samples, n_targets] ä¸€èˆ¬æ˜¯äºŒç»´æ•°ç»„ã€‚

# In[73]:


sns.pairplot(iris)  # å¿«é€ŸæŸ¥çœ‹å„ç‰¹å¾å…³ç³»


# In[2]:


X_iris = iris.drop('species', axis=1)
X_iris.shape
Y_iris = iris['species']
Y_iris.shape


# In[ ]:


# ### 5.2.2ã€€Scikit-Learnçš„è¯„ä¼°å™¨API

# **1. APIåŸºç¡€çŸ¥è¯†**
#
# Scikit-Learn è¯„ä¼°å™¨ API çš„å¸¸ç”¨æ­¥éª¤å¦‚ä¸‹æ‰€ç¤ºï¼ˆåé¢ä»‹ç»çš„ç¤ºä¾‹éƒ½æ˜¯æŒ‰ç…§è¿™äº›æ­¥éª¤è¿›è¡Œçš„
#
# (1) é€šè¿‡ä» Scikit-Learn ä¸­å¯¼å…¥é€‚å½“çš„è¯„ä¼°å™¨ç±»ï¼Œ*é€‰æ‹©æ¨¡å‹ç±»*ã€‚
#
# (2) ç”¨åˆé€‚çš„æ•°å€¼å¯¹æ¨¡å‹ç±»è¿›è¡Œå®ä¾‹åŒ–ï¼Œé…ç½®æ¨¡å‹è¶…å‚æ•°ï¼ˆhyperparameterï¼‰ã€‚
#
# (3) æ•´ç†æ•°æ®ï¼Œé€šè¿‡å‰é¢ä»‹ç»çš„æ–¹æ³•è·å–ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡æ•°ç»„ã€‚
#
# (4) è°ƒç”¨æ¨¡å‹å®ä¾‹çš„ fit() æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œæ‹Ÿåˆã€‚
#
# (5) å¯¹æ–°æ•°æ®åº”ç”¨æ¨¡å‹ï¼š
#
# - åœ¨æœ‰ç›‘ç£å­¦ä¹ æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ predict() æ–¹æ³•é¢„æµ‹æ–°æ•°æ®çš„æ ‡ç­¾ï¼›
# - åœ¨æ— ç›‘ç£å­¦ä¹ æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ transform()æˆ– predict()æ–¹æ³•è½¬æ¢æˆ–æ¨æ–­æ•°æ®çš„æ€§è´¨
#
# ä¸‹é¢æŒ‰ç…§æ­¥éª¤æ¥æ¼”ç¤ºå‡ ä¸ªä½¿ç”¨äº†æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•å’Œæ— ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ç¤ºä¾‹ã€‚

# In[ ]:


# **2. æœ‰ç›‘ç£å­¦ä¹ ç¤ºä¾‹ï¼šç®€å•çº¿æ€§å›å½’**

# (0) å¯¼å…¥æ•°æ®

# In[3]:


rng = np.random.RandomState(42)
x = 10 * rng.rand(50)
y = 2 * x - 1 + rng.rand(50)
plt.scatter(x, y)


# (1) é€‰æ‹©æ¨¡å‹ç±»

# In[5]:


# (2) é…ç½®æ¨¡å‹å®ä¾‹å¯¹è±¡çš„è¶…å‚æ•°

# In[6]:


model = LinearRegression(fit_intercept=True)  # å¸¦æˆªè·


# (3) æ•´ç†æ•°æ®ä¸ºç‰¹å¾çŸ©é˜µå’Œç›®æ ‡æ•°ç»„

# In[8]:


X = x[:, np.newaxis]  # è½¬æ¢æˆåˆ—
X.shape


# (4) æ¨¡å‹æ‹Ÿåˆæ•°æ®
# - fit() å‘½ä»¤ä¼šåœ¨æ¨¡å‹å†…éƒ¨è¿›è¡Œå¤§é‡è¿ç®—ï¼Œè¿ç®—ç»“æœå°†å­˜å‚¨åœ¨æ¨¡å‹å±æ€§ä¸­
# - è·å¾—çš„æ¨¡å‹å‚æ•°éƒ½æ˜¯åç¼€_çš„

# In[9]:


model.fit(X, y)


# In[10]:


model.coef_  # æ–œç‡


# In[11]:


model.intercept_  # æˆªè·


# (5) é¢„æµ‹æ–°æ•°æ®

# In[12]:


xfit = np.linspace(-1, 11)
Xfit = xfit[:, np.newaxis]
yfit = model.predict(Xfit)
plt.scatter(x, y)
plt.plot(xfit, yfit)  # è¿æ¥æˆçº¿


# **3. æœ‰ç›‘ç£å­¦ä¹ ç¤ºä¾‹ï¼šé¸¢å°¾èŠ±æ•°æ®åˆ†ç±»**ï¼Œé«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼ˆGaussian naive Bayesï¼‰

# In[14]:


iris.loc[:, :'species']


# å‡è®¾æ ·æœ¬æ˜¯ç”±é«˜æ–¯åˆ†å¸ƒäº§ç”Ÿçš„ï¼Œç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•åˆ†ç±»ã€‚
#
# é€šå¸¸éƒ½ä¼šå…ˆç”¨è¿™ä¸ªç®—æ³•ï¼Œå› ä¸ºé€Ÿåº¦å¾ˆå¿«ï¼Œä¸éœ€è¦è¶…å‚æ•°é…ç½®ã€‚ç„¶åä¼˜åŒ–

#

# In[20]:


# (0)å¯¼å…¥æ•°æ®
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_iris, Y_iris, random_state=1)
print(Xtrain, Ytrain)


# In[22]:


model = GaussianNB()                       # 2.åˆå§‹åŒ–æ¨¡å‹
model.fit(Xtrain, Ytrain)                  # 3.ç”¨æ¨¡å‹æ‹Ÿåˆæ•°æ®
y_model = model.predict(Xtest)             # 4.å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹


# In[24]:


accuracy_score(y_model, Ytest)  # æ£€æŸ¥æ¨¡å‹å‡†ç¡®ç‡


# **4. æ— ç›‘ç£å­¦ä¹ ç¤ºä¾‹ï¼šé¸¢å°¾èŠ±æ•°æ®é™ç»´**

# In[25]:


model = PCA(n_components=2)      # 2.è®¾ç½®è¶…å‚æ•°ï¼Œåˆå§‹åŒ–æ¨¡å‹
model.fit(X_iris)                # 3.æ‹Ÿåˆæ•°æ®ï¼Œæ³¨æ„è¿™é‡Œä¸ç”¨yå˜é‡
X_2D = model.transform(X_iris)   # 4. å°†æ•°æ®è½¬æ¢ä¸ºäºŒç»´


# In[26]:


iris['PCA1'] = X_2D[:, 0]
iris['PCA2'] = X_2D[:, 1]


# In[27]:


iris


# In[29]:


sns.lmplot(x="PCA1", y="PCA2", hue='species', data=iris, fit_reg=False)


# å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡PCA1/2 å¯¹setosaå¾ˆå¥½çš„åˆ†ç±»ï¼Œ virå’Œverç±»æœ‰ç‚¹ä¸å¤ªå¥½

# **5. æ— ç›‘ç£å­¦ä¹ ç¤ºä¾‹ï¼šé¸¢å°¾èŠ±æ•°æ®èšç±»**ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹

# In[31]:


model = GaussianMixture(
    n_components=3, covariance_type='full')  # 2.è®¾ç½®è¶…å‚æ•°ï¼Œåˆå§‹åŒ–æ¨¡å‹
model.fit(X_iris)  # 3.æ‹Ÿåˆæ•°æ®ï¼Œæ³¨æ„ä¸éœ€è¦yå˜é‡
y_gmm = model.predict(X_iris)  # 4. ç¡®å®šç°‡æ ‡ç­¾
y_gmm


# In[32]:


iris['cluster'] = y_gmm
iris


# In[33]:


sns.lmplot(x="PCA1", y="PCA2", hue='species',
           col='cluster', data=iris, fit_reg=False)


# å›¾ä¸Šèšç±»å’Œç»“æœå’ŒPCAçœ‹æ¥å·®ä¸å¤šï¼Œåœ¨virå’Œverä¹‹é—´è¿˜æ˜¯æœ‰ç‚¹æ¨¡ç³Š

# ### 5.2.3ã€€åº”ç”¨ï¼šæ‰‹å†™æ•°å­—æ¢ç´¢

# 1. åŠ è½½å¹¶å¯è§†åŒ–æ‰‹å†™æ•°å­—

# In[34]:


digits = load_digits()
digits.images.shape


# In[39]:


fig, axes = plt.subplots(8, 8, figsize=(8, 8))
fig.subplots_adjust(hspace=0, wspace=0)
for i, ax in enumerate(axes.flat):  # axesäºŒç»´æ‰å¹³åŒ–ï¼Œæ–¹ä¾¿éå†
    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')
    # ç›¸å¯¹åæ ‡x,y ,æ˜¾ç¤ºæ–‡å­—ï¼Œ transformè¡¨ç¤ºä»¥å­axè¡¨ç¤ºè€Œä¸æ˜¯æ•´å›¾
    ax.text(0.05, 0.05, str(digits.target[i]),
            transform=ax.transAxes, color='green')


# æ„é€ ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡æ•°ç»„

# In[43]:


X = digits.data  # å·²ç»æ„å»ºå¥½
X.shape


# In[60]:


y = digits.target
y.shape


# **2. æ— ç›‘ç£å­¦ä¹ ï¼šé™ç»´**
#
# æˆ‘æƒ³ç‰¹å¾å¯è§†åŒ–å…ˆçœ‹çœ‹ï¼Œ ä½†æ˜¯64ç»´ ï¼Œç»´åº¦å¤ªé«˜äº†ï¼Œéœ€è¦é™ç»´åˆ°ä½ç»´åº¦ï¼Œé€šè¿‡ æµå½¢å­¦ä¹ çš„losmapè¿›è¡Œé™ç»´ï¼š

# In[47]:


iso = Isomap(n_components=2)
iso.fit(digits.data)
data_projected = iso.transform(digits.data)
data_projected.shape


# In[56]:


plt.scatter(x=data_projected[:, 0], y=data_projected[:, 1], c=digits.target,
            edgecolors='none', alpha=0.5, cmap=plt.colormaps['magma'].resampled(10))
plt.colorbar(label='digit label', ticks=range(10))  # colorbaræ˜¾ç¤ºä¿¡æ¯
plt.clim(-0.5, 9.5)


# è™½ç„¶çœ‹èµ·æ¥å¾ˆæ··æ·†ï¼Œæ¯•ç«Ÿ10ä¸ªç±»å‹ï¼Œä½†å¯ä»¥çœ‹åˆ°æ˜æ˜¾æœ‰äº›é¢œè‰²ä¸ä¼šé‡å ï¼Œæ¯”å¦‚é»„è‰²9 å’Œ 6ï¼Œ å³åŒºåˆ†æ¯”è¾ƒå¥½ï¼Œå¸¸è¯†æ¥è®²ä¹Ÿæ˜¯æ›´å®¹æ˜“åŒºåˆ†ã€‚

# ç”¨æœ‰ç›‘ç£çš„å­¦ä¹ æ¥çœ‹çœ‹ï¼Œæ¯•ç«Ÿæ— ç›‘ç£çœ‹èµ·æ¥è¿˜å¯ä»¥äº†

# **3. æ•°å­—åˆ†ç±»ï¼šæœ‰ç›‘ç£çš„å­¦ä¹ **

# In[63]:


Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)


# In[65]:


model = GaussianNB()
model.fit(Xtrain, ytrain)
y_model = model.predict(Xtest)


# In[67]:


accuracy_score(ytest, y_model)


# å‡†ç¡®ç‡æŒ‡æ ‡ä¸å¤Ÿï¼Œéœ€è¦å…¶ä»–æŒ‡æ ‡ï¼šæ··æ·†çŸ©é˜µ

# In[68]:


mat = confusion_matrix(ytest, y_model)
sns.heatmap(mat, square=True, annot=True, cbar=False)
plt.xlabel('predicted value')
plt.ylabel('true value')


# å¯ä»¥çœ‹åˆ°ï¼Œä¸»è¦æ˜¯å°†2è¯¯åˆ¤æˆ8ã€‚ ç°åœ¨æˆ‘ä»¬ä»å›¾åƒä¸Šæ ‡è¯†çœ‹ä¸‹è¯¯åˆ¤çš„ã€‚

# In[90]:


print(Xtest.shape)
fig, axes = plt.subplots(10, 10, figsize=(8, 8))
test_images = Xtest.reshape(-1, 8, 8)
for i, ax in enumerate(axes.flat):
    ax.imshow(test_images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(y_model[i]),
            transform=ax.transAxes,
            color='green' if (ytest[i] == y_model[i]) else 'red')  # æ­£ç¡®çš„ç»¿è‰²æ ‡ç­¾ï¼Œ


# ä»…æŸ¥çœ‹åˆ†ç±»é”™è¯¯çš„å›¾åƒ

# In[91]:


print(Xtest.shape, ytest.shape)
fig, axes = plt.subplots(10, 10, figsize=(8, 8))
mask = ytest != y_model  # å¾—åˆ°é”™è¯¯çš„å¸ƒå°”ç´¢å¼•
print(test_images[mask].shape)
Xtest_error = Xtest[mask]
error_ymodel = y_model[mask]
error_ytest = ytest[mask]
error_images = Xtest_error.reshape(-1, 8, 8)

plt.setp(axes, xticks=[], yticks=[])  # ä¸€æ¬¡æ€§å»æ‰å…¨éƒ¨åæ ‡è½´
for i, ax in enumerate(axes.flat[: len(error_images)]):
    ax.imshow(error_images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(error_ymodel[i]),
            transform=ax.transAxes, c='red')  # é¢„æµ‹å€¼
    ax.text(0.05, 0.5, str(error_ytest[i]),
            transform=ax.transAxes, c='green')  # åŸå€¼


# ## 5.3ã€€è¶…å‚æ•°ä¸æ¨¡å‹éªŒè¯
# **äº¤å‰æ£€éªŒæ–¹æ³•**è°ƒæ•´å‚æ•°è‡³å…³é‡è¦ï¼Œè¿™æ ·åšå¯ä»¥é¿å…è¾ƒå¤æ‚ / çµæ´»æ¨¡å‹å¼•èµ·çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
#
# æ¨¡å‹éªŒè¯ï¼šé¢„æµ‹å€¼å’Œå®é™…å€¼å·®å¼‚ã€‚ äº¤å‰éªŒè¯ï¼
#
# è¶…å‚æ•°ä¼˜åŒ–ï¼šéªŒè¯æ›²çº¿ã€‚

# 1. é”™è¯¯çš„æ¨¡å‹éªŒè¯æ–¹æ³•

# In[92]:


iris = load_iris()
X = iris.data
y = iris.target


# In[93]:


model = KNeighborsClassifier(n_neighbors=1)


# In[94]:


model.fit(X, y)
y_model = model.predict(X)


# In[96]:


accuracy_score(y, y_model)  # æ˜¾ç„¶é”™è¯¯å‡†ç¡®ç‡ï¼Œ


# 2. æ¨¡å‹éªŒè¯æ­£ç¡®æ–¹æ³•ï¼šç•™å‡ºé›†

# In[98]:


# æ¯ä¸ªæ•°æ®é›†åˆ†ä¸€åŠæ•°æ®
X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
                                  train_size=0.5)
# ç”¨æ¨¡å‹æ‹Ÿåˆè®­ç»ƒæ•°æ®
model.fit(X1, y1)

# åœ¨æµ‹è¯•é›†ä¸­è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡
y2_model = model.predict(X2)
accuracy_score(y2, y2_model)


# 3. äº¤å‰æ£€éªŒ
#
# å°±æ˜¯å°±è¡Œå‡ è½®å®éªŒï¼Œä½¿å¾—å……åˆ†ä½œä¸º è®­ç»ƒå’Œæµ‹è¯•ã€‚  å‡†ç¡®ç‡å–å¹³å‡

# In[102]:


y2_model = model.fit(X1, y1).predict(X2)
y1_model = model.fit(X2, y2).predict(X1)
accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)


# è‡ªåŠ¨è¿›è¡Œå‡ è½®ï¼Œæ— é¡»æ‰‹åŠ¨ä»£ç 

# In[101]:


cross_val_score(model, X, y, cv=5)


# ### 5.3.2ã€€é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼šéªŒè¯æ›²çº¿
#
# å‡å¦‚æ¨¡å‹æ•ˆæœä¸å¥½ï¼Œåº”è¯¥å¦‚ä½•æ”¹å–„ï¼Ÿ
#
# **é—®é¢˜çš„ç­”æ¡ˆå¾€å¾€ä¸ç›´è§‰ç›¸æ‚–**
#
# æ¢ä¸€ç§æ›´å¤æ‚çš„æ¨¡å‹æœ‰æ—¶å¯èƒ½äº§ç”Ÿæ›´å·®çš„ç»“æœï¼Œå¢åŠ æ›´å¤š çš„è®­ç»ƒæ ·æœ¬ä¹Ÿæœªå¿…èƒ½æ”¹å–„æ€§èƒ½ï¼æ”¹å–„æ¨¡å‹èƒ½åŠ›çš„é«˜ä½ï¼Œæ˜¯åŒºåˆ†æœºå™¨å­¦ä¹ å®è·µè€…æˆåŠŸä¸å¦ çš„æ ‡å¿—ã€‚

# **1. åå·®ä¸æ–¹å·®çš„å‡è¡¡**

# æ˜¾ç„¶ï¼Œå·¦ä¾§å›¾æ¬ æ‹Ÿåˆï¼ˆé«˜åå·®ï¼‰ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½ä¸å¥½ï¼‰ï¼Œå³è¾¹è¿‡æ‹Ÿåˆï¼ˆé«˜æ–¹å·®ï¼‰ï¼ˆè®­ç»ƒé›†è¿œè¿œå¤§äºæµ‹è¯•é›†è¡¨ç°ï¼‰

# ![å›¾ç‰‡.png](attachment:2df6c839-3c93-42a5-853a-26b97fe763da.png)

# é€šè¿‡ä¸ç®¡è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ï¼Œæ¥è¾¾åˆ°æŠ˜ä¸­ã€‚  ä¸åŒæ¨¡å‹è°ƒæ•´æ–¹æ³•ä¸åŒã€‚

# ![å›¾ç‰‡.png](attachment:5e58d71e-e755-40f1-8592-41f4e796dcb0.png)

# **2. Scikit-LearnéªŒè¯æ›²çº¿**
# å¦‚ä½•è®¡ç®—è¿™æ ·çš„éªŒè¯æ›²çº¿å›¾ï¼Œä¸Šå›¾ã€‚

# ä¾‹å­ï¼šnæ¬¡å¤šé¡¹å¼å›å½’

# In[3]:


def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree),
                         LinearRegression(**kwargs))


# In[4]:


def make_data(N, err=1.0, rseed=1):
    # éšæœºè½´æ ·æ•°æ®
    rng = np.random.RandomState(rseed)
    X = rng.rand(N, 1) ** 2
    y = 10 - 1. / (X.ravel() + 0.1)
    if err > 0:
        y += err * rng.randn(N)
    return X, y


X, y = make_data(40)
print(X.shape, y.shape)


# In[5]:


seaborn.set()  # è®¾ç½®å›¾å½¢æ ·å¼

X_test = np.linspace(-0.1, 1.1, 500)[:, None]
plt.scatter(X.ravel(), y, color='black')  # ravelå‡½æ•°æ‰å¹³åŒ–ï¼Œè¿”å›æ•°æ®è§†å›¾ï¼Œ
axis = plt.axis()
for degree in [1, 3, 5]:
    y_test = PolynomialRegression(degree).fit(
        X, y).predict(X_test)  # degreeæ§åˆ¶æ¬¡æ•°
    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))
plt.xlim(-0.1, 1.0)
plt.ylim(-2, 12)
plt.legend(loc='best')


#

# **é—®é¢˜ ï¼šå“ªä¸ªå›å½’æ›²çº¿æ›´å¥½ï¼Ÿ å³åšéªŒè¯æ›²çº¿ï¼ŒéªŒè¯å¾—åˆ†**

# ```
# validation_curve(
#     estimator,  # éœ€è¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆå¦‚çº¿æ€§å›å½’ã€SVMç­‰ï¼‰
#     X, y,       # è®­ç»ƒæ•°æ®å’Œç›®æ ‡å€¼
#     param_name, # è¦è°ƒæ•´çš„è¶…å‚æ•°åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
#     param_range,# è¶…å‚æ•°çš„å–å€¼èŒƒå›´ï¼ˆæ•°ç»„/åˆ—è¡¨ï¼‰
#     scoring=None, # è¯„åˆ†æŒ‡æ ‡ï¼ˆé»˜è®¤ä¸ºæ¨¡å‹çš„é»˜è®¤è¯„åˆ†æ–¹æ³•ï¼Œå¦‚ R^2ï¼‰
#     cv=5,       # äº¤å‰éªŒè¯çš„æŠ˜æ•°ï¼ˆé»˜è®¤ä¸º 5 æŠ˜äº¤å‰éªŒè¯ï¼‰
#     n_jobs=None # å¹¶è¡Œè®¡ç®—çš„ CPU æ ¸æ•°ï¼ˆNone è¡¨ç¤ºé»˜è®¤ï¼‰
# )
# ```

# In[9]:


degree = np.arange(0, 21)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,
                                          param_name='polynomialfeatures__degree',
                                          param_range=degree, cv=5)
# train_score(21,5), æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªdegreeå‚æ•°çš„5æ¬¡äº¤å‰éªŒè¯å¾—åˆ†
plt.plot(degree, np.median(train_score, 1), color='blue',
         label='training score')  # å–5æ¬¡äº¤å‰éªŒè¯çš„å¹³å‡å€¼åˆ†æ•°
plt.plot(degree, np.median(val_score, 1),
         color='red', label='validation score')
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score')


# ç»“è®ºï¼š
# 1. è®­ç»ƒå¾—åˆ†ä¸€å®šå¤§äºéªŒè¯å¾—åˆ†ï¼Œå¤æ‚åº¦è¶Šé«˜ï¼Œè®­ç»ƒå¾—åˆ†ä¸Šå‡ï¼Œ ä½†éªŒè¯å¾—åˆ†ä¼šç”±äºè¿‡æ‹Ÿåˆéª¤é™ã€‚
# 2. 3æ¬¡å¤šé¡¹å¼æ˜¯ å¹³è¡¡ åå·®å’Œæ–¹å·® æœ€å¥½çš„ç‚¹ã€‚
#
# ç°åœ¨æ¥çœ‹ä¸‹é•¿ä»€ä¹ˆæ ·å­

# In[14]:


print(X.shape, y.shape)
plt.scatter(X.ravel(), y)
lim = plt.axis()
y_test = PolynomialRegression(3).fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test)
plt.axis(lim)


# ç¡®å®ï¼Œ3æ¬¡å¤šé¡¹å¼æ‹Ÿåˆæ•ˆæœå¾ˆå¥½ï¼

# ### 5.3.3ã€€å­¦ä¹ æ›²çº¿
# å½±å“æ¨¡å‹å¤æ‚åº¦çš„å¦ä¸€ä¸ªé‡è¦å› ç´ æ˜¯æœ€ä¼˜æ¨¡å‹å¾€å¾€å—åˆ°è®­ç»ƒæ•°æ®é‡çš„å½±å“

# In[21]:


X2, y2 = make_data(200)  # ç”Ÿæˆäº†5å€çš„æ•°æ®degree = np.arange(21)
train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,
                                            param_name='polynomialfeatures__degree',
                                            param_range=degree,  cv=7)

# 200æ•°æ®çš„éªŒè¯æ›²çº¿
plt.plot(degree, np.median(train_score2, 1), color='blue',
         label='training score')
plt.plot(degree, np.median(val_score2, 1),
         color='red', label='validation score')
# 50å°æ•°æ®çš„éªŒè¯æ›²çº¿: è™šçº¿
plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3,
         linestyle='dashed')
plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3,
         linestyle='dashed')
plt.legend(loc='lower center')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score')
plt.scatter(X2.ravel(), y2)


# ç»“è®ºï¼š
# 1. å¤§æ•°æ®æ”¯æŒæ›´é«˜çš„å¤æ‚å¤šé¡¹å¼ï¼Œè€Œå°æ•°æ®åœ¨ç»´åº¦13å·¦å³å°±è¿‡æ‹Ÿåˆéª¤é™äº†ã€‚
#
# è¿™ç§é€šè¿‡æ‰©å¤§æ•°æ®è§„æ¨¡å¯¹æ¯”æ¨¡å‹å¾—åˆ†ï¼Œ ç§°ä¸ºå­¦ä¹ æ›²çº¿ã€‚
#
# å› æ­¤ï¼Œ**å­¦ä¹ æ›²çº¿åæ˜  æ•°æ®é‡ å¯¹æ¨¡å‹å½±å“ã€‚** **ï¼ˆéªŒè¯æ›²çº¿åæ˜  è¶…å‚æ•°ï¼ˆå¤æ‚åº¦ï¼‰å¯¹ æ¨¡å‹å¾—åˆ†çš„å½±å“ï¼‰ã€‚**
#
# æ¨¡å‹å¾—åˆ†æ˜¯å¹³è¡¡è®­ç»ƒå¾—åˆ†å’ŒéªŒè¯å¾—åˆ†ã€‚
#
# å­¦ä¹ æ›²çº¿æœ€æœ€é‡è¦çš„å°±æ˜¯ï¼Œéšç€æ ·æœ¬çš„å¢åŠ ï¼Œæ¨¡å‹å¾—åˆ†ä¸€å®šä¼šæ”¶æ•›ï¼Œå³å†å¤šæ ·ä¹Ÿæ²¡æœ‰ç”¨

# ![å›¾ç‰‡.png](attachment:41572382-d8e4-4e72-8cc0-0d1049591428.png)

# > å› æ­¤ï¼Œå³æ¨¡å‹èƒ½åŠ›ç”¨å°½äº†ï¼Œ åªèƒ½æ¢æ¨¡å‹æ‰èƒ½äº†

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿

# ---
# | å‚æ•°           | ç±»å‹                 | è¯´æ˜ |
# |--------------|------------------|------|
# | `estimator`  | ä¼°è®¡å™¨å¯¹è±¡        | éœ€è¦è¯„ä¼°çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ `SVC()`ã€`RandomForestClassifier()`ï¼‰ã€‚ |
# | `X`          | æ•°ç»„æˆ–çŸ©é˜µ        | è®­ç»ƒæ•°æ®çš„ç‰¹å¾çŸ©é˜µã€‚ |
# | `y`          | æ•°ç»„              | è®­ç»ƒæ•°æ®çš„ç›®æ ‡å˜é‡ï¼ˆæ ‡ç­¾ï¼‰ã€‚ |
# | `train_sizes` | æ•°ç»„ (é»˜è®¤ 5 ä¸ªç‚¹) | è®­ç»ƒé›†çš„ä¸åŒå­é›†å¤§å°ï¼ˆå¦‚ `np.linspace(0.1, 1.0, 5)` ä»£è¡¨ 10% åˆ° 100% ï¼‰ã€‚ |
# | `cv`         | int / äº¤å‰éªŒè¯ç­–ç•¥ | äº¤å‰éªŒè¯çš„æŠ˜æ•°æˆ–äº¤å‰éªŒè¯æ–¹æ³•ï¼ˆå¦‚ `5` æˆ– `KFold(n_splits=5)`ï¼‰ã€‚ |
# | `scoring`    | str / callable   | è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚ `"accuracy"`ã€`"neg_mean_squared_error"`ï¼‰ã€‚ |
# | `n_jobs`     | int             | å¹¶è¡Œè®¡ç®—çš„ä½œä¸šæ•°ï¼ˆ`-1` ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒï¼‰ã€‚ |
# | `shuffle`    | bool            | æ˜¯å¦åœ¨åˆ’åˆ†æ•°æ®å‰è¿›è¡Œæ´—ç‰Œï¼ˆé»˜è®¤ `False`ï¼‰ã€‚ |
# | `random_state` | int / None     | éšæœºç§å­ï¼ˆé€‚ç”¨äº `shuffle=True` æ—¶ï¼‰ã€‚ |
# | `verbose`    | int             | è¯¦ç»†ç¨‹åº¦ï¼ˆ`0` ä¸ºä¸è¾“å‡ºä¿¡æ¯ï¼‰ã€‚ |
# | `return_times` | bool           | æ˜¯å¦è¿”å›è®­ç»ƒå’Œæµ‹è¯•çš„æ‰§è¡Œæ—¶é—´ï¼ˆé»˜è®¤ä¸º `False`ï¼‰ã€‚ |
# ---
#
#
#

# In[25]:


fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for i, degree in enumerate([2, 9]):
    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),
                                         X, y, cv=7,
                                         train_sizes=np.linspace(0.3, 1, 25))  # è®­ç»ƒé›†å¤§å°ï¼Œåˆ†ä¸º25ä»½ ä»30%åˆ°1

    print(N, train_lc.shape, val_lc.shape)  # æ¯è¡Œä½ä¸€ä¸ªæ•°æ®é‡åœ¨äº¤å‰éªŒè¯çš„å¾—åˆ†ï¼ŒNè®­ç»ƒæ ·æœ¬çš„å®é™…æ•°é‡ã€‚
    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')
    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')
    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1], color='gray',
                 linestyle='dashed')  # ç»˜åˆ¶æ°´å¹³çº¿ï¼ˆhorizontal linesï¼‰

    ax[i].set_ylim(0, 1)
    ax[i].set_xlim(N[0], N[-1])
    ax[i].set_xlabel('training size')
    ax[i].set_ylabel('score')
    ax[i].set_title('degree = {0}'.format(degree), size=14)
    ax[i].legend(loc='best')


# **ç»“è®ºï¼š**
# 1. è®­ç»ƒå¾—åˆ†ä¸éªŒè¯å¾—åˆ†ä¸€å®šä¼šæ”¶æ•›éšç€æ•°æ®é‡å˜åŒ–ã€‚
# 2. é«˜ç»´åº¦å³é«˜å¤æ‚åº¦çš„æ¨¡å‹éœ€è¦æ›´å¤šçš„æ•°æ®é‡ï¼Œç›¸å¯¹ä½çº¬åº¦è€Œè¨€

# ### 5.3.4ã€€éªŒè¯å®è·µï¼šç½‘æ ¼æœç´¢
# å®é™…ä¸­ï¼Œæ¨¡å‹ä¼šæœ‰å¤šä¸ªè¶…å‚æ•°ï¼Œå­¦ä¹ æ›²çº¿å’ŒéªŒè¯æ›²çº¿ä¼šå˜ä¸ºå¤šç»´æ›²é¢ã€‚ è¿™æ—¶ï¼Œæ‰¾å‡ºæœ€ä¼˜å¾—åˆ†ç‚¹ ä¸å®¹æ˜“ï¼
#
# grid_search å·¥å…·ğŸ†—

# In[29]:


# å¤šä¸ªè¶…å‚æ•°ï¼šç»´åº¦ï¼Œæ˜¯å¦æˆªè·
param_grid = {'polynomialfeatures__degree': np.arange(21),
              'linearregression__fit_intercept': [True, False]}

grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)
grid.fit(X, y)


# In[31]:


grid.best_params_  # è·å¾—æœ€ä¼˜è¶…å‚æ•°


# In[1]:


model = grid.best_estimator_

plt.scatter(X.ravel(), y)
lim = plt.axis()
y_test = model.fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test)
plt.axis(lim)


# In[ ]:


# ## 5.4ã€€ç‰¹å¾å·¥ç¨‹
# å°±æ˜¯æ¸…ç†å¹²å‡€æ•°æ®ï¼Œå˜ä¸ºç‰¹å¾çŸ©é˜µã€‚

# ### 5.4.1ã€€åˆ†ç±»ç‰¹å¾

# æŠŠåˆ†ç±»æ ‡ç­¾01åŒ–ï¼Œç‹¬çƒ­ç¼–ç ã€‚

# In[10]:


data = [
    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},
    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},
    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},
    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}
]


# DictVectorizerè‡ªåŠ¨æŠŠ æ‰€æœ‰å­—ç¬¦ä¸²æ ‡ç­¾è½¬åŒ–ä¸ºOne-hotï¼Œ ä½†ç»´åº¦éª¤å¢ï¼

# In[5]:


vec = DictVectorizer(sparse=False, dtype=int)
vec.fit_transform(data)


# In[9]:


vec.get_feature_names_out()  # å¾—åˆ°æ¯ä¸ªæ‹†åˆ†åçš„è§£é‡Šï¼


# ç”±äºå¤ªå¤š0é€šè¿‡ç¨€ç–çŸ©é˜µå­˜å‚¨1

# In[12]:


vec = DictVectorizer(sparse=True, dtype=int)
vec.fit_transform(data)


# ### 5.4.2ã€€æ–‡æœ¬ç‰¹å¾
# æŠŠä¸€ä¸²æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç»„æ•°å€¼ã€‚
#
#

# æœ€ç®€å•çš„æ–¹å¼ï¼šå°±æ˜¯å•è¯ç»Ÿè®¡ï¼Œå•è¯ä¸ºåˆ—ï¼Œä¸€è¡Œæ˜¾ç¤ºäº†ç”±å“ªå‡ ä¸ªå•è¯ç»„æˆ
#

# In[13]:


sample = ['problem of evil',
          'evil queen',
          'horizon problem']


# In[14]:


vec = CountVectorizer()
X = vec.fit_transform(sample)
X  # ç¨€ç–çŸ©é˜µ


# In[17]:


pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())


# ç¼ºç‚¹å°±æ˜¯å¸¸ç”¨è¯æƒé‡å¤ªé«˜äº†ï¼Œæ˜¾ç„¶å½±å“åˆ†ç±»æ•ˆæœã€‚
#
# é€šè¿‡ TFâ€“IDFï¼ˆterm frequencyâ€“inverse document frequencyï¼Œè¯é¢‘é€†æ–‡æ¡£é¢‘ç‡ï¼‰ï¼Œé€šè¿‡å•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡æ¥è¡¡é‡å…¶æƒé‡ ã€‚
# 1. TF: å‡ºç°é¢‘ç‡é«˜è¶Šé‡è¦ ï¼ˆåœ¨å½“å‰æ–‡æ¡£ï¼‰
# 2. IDF: ä½†æ˜¯ä¸€äº›å¸¸ç”¨è¯é¢‘ç‡è¿‡é«˜ ä½†æ²¡æœ‰ä¿¡æ¯ï¼Œåè€Œä¸€äº›å°‘é‡è¯æ±‡ æ›´é‡è¦ï¼Œå¦‚ é‡å­ã€‚ï¼ˆåœ¨å¾ˆå¤šæ–‡æ¡£ï¼‰

# In[19]:


vec = TfidfVectorizer()
X = vec.fit_transform(sample)
pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())


# ### 5.4.3ã€€å›¾åƒç‰¹å¾
# åœ¨ Scikit-Image é¡¹ç›®

# ### 5.4.4ã€€è¡ç”Ÿç‰¹å¾
# æ˜¯é€šè¿‡æ•°å­¦å˜æ¢è¡ç”Ÿå‡ºæ¥çš„ï¼Œå¹¶éæ˜¯åŸæ•°æ®å­—æ®µã€‚
# å˜æ¢è¾“å…¥ã€‚

# In[20]:


x = np.array([1, 2, 3, 4, 5])
y = np.array([4, 2, 1, 3, 7])
plt.scatter(x, y)


# In[25]:


X = x[:, np.newaxis]
model = LinearRegression().fit(X, y)
yfit = model.predict(X)
plt.scatter(x, y)
plt.plot(x, yfit)


# æ˜¾ç„¶ è¿™äº›æ•°æ®ä¸èƒ½é€šè¿‡ç›´çº¿æ‹Ÿåˆ

# æ·»åŠ äº† å¹³æ–¹ï¼Œç«‹æ–¹ï¼Œã€‚ å³å°†ä¸€æ¬¡å¤šé¡¹å¼è½¬æ¢ä¸º3æ¬¡å¤šé¡¹å¼æ‹Ÿåˆã€‚
# ç‰¹å¾æ‰©å¤§åˆ°äº†3

# In[24]:


poly = PolynomialFeatures(degree=3, include_bias=False)
X2 = poly.fit_transform(X)
print(X2)


# In[27]:


model = LinearRegression().fit(X2, y)
yfit = model.predict(X2)
plt.scatter(x, y)
plt.plot(x, yfit)


# **æ€æƒ³ï¼ï¼šä¸æ”¹å˜æ¨¡å‹ï¼Œè€Œæ˜¯æ”¹å˜è¾“å…¥ï¼Œæ‰©å……è¾“å…¥ï¼Œ æé«˜æ•ˆæœ**ã€‚ å¸¸ç§°ä¸ºåŸºå‡½æ•°å›å½’

# ### 5.4.5ã€€ç¼ºå¤±å€¼å¡«å……

# In[30]:


X = np.array([[nan, 0,   3],
              [3,   7,   9],
              [3,   5,   2],
              [4,   nan, 6],
              [8,   8,   1]])
y = np.array([14, 16, -1,  8, -5])


# In[34]:


imp = SimpleImputer(strategy='mean')  # åˆ—å‡å€¼å¡«å……
X2 = imp.fit_transform(X)
X2


# In[36]:


model = LinearRegression().fit(X2, y)
model.predict(X2)


# ### 5.4.6ã€€ç‰¹å¾ç®¡é“
# å°†ä¸Šè¿°æ–¹æ³•è¿åœ¨ä¸€èµ·å¤„ç†æ—¶å€™å¯ä»¥ç®€åŒ–ï¼Œæ¯”å¦‚ï¼š
# 1. å‡å€¼å¡«å……ç¼ºå¤±
# 2. è¡ç”Ÿç‰¹å¾åˆ°äºŒæ¬¡
# 3. æ‹Ÿåˆçº¿æ€§å›å½’
#
# é€šè¿‡ç®¡é“pipelineå¯¹è±¡

# In[40]:


model = make_pipeline(SimpleImputer(strategy='mean'),
                      PolynomialFeatures(degree=2),
                      LinearRegression())
model.fit(X, y)
y


# ## 5.5 æœ´ç´ è´å¶æ–¯
# ç®€å•çš„æ ¹æºåœ¨äºï¼Œæ ¹æ®æ•°æ®ç›´æ¥è®¡ç®—ï¼Œä¸éœ€è¦å­¦ä¹ è¿­ä»£

# In[ ]:


data = fetch_20newsgroups()
data.head()


# ### 5.5.1ã€€è´å¶æ–¯åˆ†ç±»

# In[1]:


sns.set()


# ### 5.5.2ã€€é«˜æ–¯æœ´ç´ è´å¶æ–¯
# å‡è®¾æ¯ä¸ªæ•°æ®ï¼ˆx,yï¼‰æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå³çŸ¥é“æ¨¡å‹ä½†ä¸çŸ¥é“å¤æ‚åº¦å‚æ•°ï¼šå‡å€¼ã€æ–¹å·®ã€‚
#
# é«˜æ–¯åˆ†å¸ƒåœ¨é«˜æ–¯æœ´ç´ è´å¶æ–¯ä¸­åªæ˜¯ä¸€ä¸ªå‡è®¾ï¼Œå‡è®¾æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾æ•°æ®æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå³ä¼¼ç„¶å‡½æ•°ã€‚è¿™ä¸ªå‡è®¾ç®€åŒ–äº†æ¨¡å‹ï¼Œä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨ç®€å•çš„ **å‡å€¼å’Œæ–¹å·®** æ¥ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾åˆ†å¸ƒã€‚
#
#
# åœ¨è¿›è¡Œ **å‚æ•°ä¼°è®¡** æ—¶ï¼Œè´å¶æ–¯ä¼°è®¡å’Œæå¤§ä¼¼ç„¶ä¼°è®¡åªæ˜¯é€šè¿‡æ•°æ®æ¥ä¼°ç®—æ¨¡å‹å‚æ•°ï¼ˆå‡å€¼ã€æ–¹å·®ç­‰ï¼‰ï¼Œè€Œä¸éœ€è¦åœ¨ä¼°è®¡è¿‡ç¨‹ä¸­æ˜ç¡®å…³æ³¨æ•´ä¸ªé«˜æ–¯åˆ†å¸ƒæ¨¡å‹ã€‚ç®€è€Œè¨€ä¹‹ï¼Œ**ä½ éœ€è¦å…³æ³¨çš„æ˜¯å¦‚ä½•æ ¹æ®æ•°æ®æ¨æ–­å‡ºåˆé€‚çš„å‚æ•°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒæ¨¡å‹çš„å…·ä½“å½¢å¼**ã€‚

# ---
# > æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ vs. è´å¶æ–¯ä¼°è®¡ï¼ˆMAPï¼‰ åœ¨é«˜æ–¯æœ´ç´ è´å¶æ–¯ä¸­çš„åŒºåˆ«
#
# æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜ **MLE å’Œ è´å¶æ–¯ä¼°è®¡ï¼ˆMAPï¼‰** åœ¨ **é«˜æ–¯æœ´ç´ è´å¶æ–¯** ä¸­çš„ **å‚æ•°è®¡ç®—åŒºåˆ«**ã€‚
#
#
# å‡è®¾ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„äºŒåˆ†ç±»æ•°æ®é›†
# æˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€äº›æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªç‰¹å¾ `x`ï¼Œç±»åˆ« `y` åªæœ‰ä¸¤ç§ï¼š**ç±»åˆ« 0** å’Œ **ç±»åˆ« 1**ã€‚
#
#  æ•°æ®å¦‚ä¸‹ï¼š
#
# | `x`  | `y` |
# |------|-----|
# | 2.0  | 0   |
# | 2.2  | 0   |
# | 1.8  | 0   |
# | 3.0  | 1   |
# | 3.2  | 1   |
# | 2.8  | 1   |
#
# ä»»åŠ¡ï¼šç”¨ **é«˜æ–¯åˆ†å¸ƒ** æ¥ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„å‡å€¼ `Î¼` å’Œæ–¹å·® `Ïƒ^2`ã€‚
#
# æ–¹æ³• 1ï¼šæå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰
# æå¤§ä¼¼ç„¶ä¼°è®¡ç›´æ¥ç”¨ **è®­ç»ƒæ•°æ®è®¡ç®—å‡å€¼å’Œæ–¹å·®**ï¼š
#
# - å¯¹äº **ç±»åˆ« 0**ï¼š
#   $$
#   \hat{\mu}_0 = \frac{2.0 + 2.2 + 1.8}{3} = 2.0
#   $$
#
#   $$
#   \hat{\sigma}_0^2 = \frac{(2.0 - 2.0)^2 + (2.2 - 2.0)^2 + (1.8 - 2.0)^2}{3} = \frac{0 + 0.04 + 0.04}{3} = 0.0267
#   $$
#
# - å¯¹äº **ç±»åˆ« 1**ï¼š
#   $$
#   \hat{\mu}_1 = \frac{3.0 + 3.2 + 2.8}{3} = 3.0
#   $$
#
#   $$
#   \hat{\sigma}_1^2 = \frac{(3.0 - 3.0)^2 + (3.2 - 3.0)^2 + (2.8 - 3.0)^2}{3} = \frac{0 + 0.04 + 0.04}{3} = 0.0267
#   $$
#
# **ç‰¹ç‚¹**ï¼š
# - âœ… **MLE** åªä¾èµ–äº **å·²æœ‰æ•°æ®**ï¼Œæ²¡æœ‰ä»»ä½•å…ˆéªŒä¿¡æ¯ã€‚
# - âš ï¸ **å½“æ•°æ®é‡å°‘æ—¶ï¼Œè®¡ç®—çš„æ–¹å·®å¯èƒ½ä¼šåå°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ**ã€‚
#
#
# æ–¹æ³• 2ï¼šè´å¶æ–¯ä¼°è®¡ï¼ˆMAPï¼‰
# è´å¶æ–¯ä¼°è®¡ä¼šåœ¨æ–¹å·®è®¡ç®—æ—¶ **åŠ ä¸€ä¸ªå¹³æ»‘é¡¹**ï¼Œé¿å…æ•°æ®å¤ªå°‘å¯¼è‡´è®¡ç®—å‡ºçš„æ–¹å·®è¿‡å°ï¼š
#
# $$
# \hat{\sigma}^2 = \frac{1}{N} \sum (x - \mu)^2 + \lambda
# $$
#
# æ¯”å¦‚ï¼Œæˆ‘ä»¬åŠ ä¸€ä¸ª **å°çš„å¹³æ»‘é¡¹** `Î» = 0.01`ï¼š
#
# - **ç±»åˆ« 0**ï¼š
#   $$
#   \hat{\sigma}_0^2 = 0.0267 + 0.01 = 0.0367
#   $$
#
# - **ç±»åˆ« 1**ï¼š
#   $$
#   \hat{\sigma}_1^2 = 0.0267 + 0.01 = 0.0367
#   $$
#
# **ç‰¹ç‚¹**ï¼š
# - âœ… é¿å…äº†æ–¹å·®è¿‡å°çš„é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ•°æ®å°‘æ—¶æ›´ç¨³å®šã€‚
# - âš ï¸ éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å¹³æ»‘å‚æ•° `Î»`ã€‚
#
#
# æ ¸å¿ƒåŒºåˆ«æ€»ç»“
#
# |  æ–¹æ³•  | è®¡ç®—å‡å€¼ `Î¼`  | è®¡ç®—æ–¹å·® `Ïƒ^2` | é€‚ç”¨åœºæ™¯ |
# |--------|------------------|------------------|-----------|
# | **MLE** | ç›´æ¥ç”¨æ•°æ®è®¡ç®—  | $Ïƒ^2 = \frac{1}{N} \sum (x - \mu)^2$ | æ•°æ®é‡å¤§æ—¶æ•ˆæœå¥½ï¼Œæ˜“è¿‡æ‹Ÿåˆ |
# | **MAP** | ç›´æ¥ç”¨æ•°æ®è®¡ç®—  | $Ïƒ^2 = \frac{1}{N} \sum (x - \mu)^2 + \lambda$ | æ•°æ®å°‘æ—¶æ›´ç¨³å®šï¼Œé˜²æ­¢æ–¹å·®è¿‡å° |
#
#
# æ€»ç»“
#
# - **MLE** æ˜¯çº¯æ•°æ®é©±åŠ¨çš„ï¼Œé€‚åˆå¤§æ•°æ®é‡åœºæ™¯ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´æ–¹å·®è¿‡å°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚
# - **MAP** åœ¨å°æ•°æ®é›†ä¸‹æ›´ç¨³å®šï¼Œå› ä¸ºå®ƒåŠ å…¥äº†ä¸€ä¸ªå¹³æ»‘é¡¹æ¥é˜²æ­¢æ–¹å·®è¿‡å°ã€‚
#

# ---

# #### æ¨¡å‹å­¦ä¹ 

# In[ ]:


# In[6]:


X, y = make_blobs(n_samples=100, n_features=2, centers=2,
                  random_state=2, cluster_std=1.5)
print(X.shape, y.shape)


# In[10]:


model = GaussianNB()  # é»˜è®¤ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡
model.fit(X, y)

print("æ¯ä¸ªç±»åˆ«çš„å‡å€¼:", model.theta_)  # [n_classes, n_features] æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªç±»åˆ« å„ä¸ªç‰¹å¾å‡å€¼
print("æ¯ä¸ªç±»åˆ«çš„æ–¹å·®:", model.var_)  # [n_classes, n_features] æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªç±»åˆ« å„ä¸ªç‰¹å¾æ–¹å·®ã€‚ ç‰¹å¾ç‹¬ç«‹


# #### é¢„æµ‹å¯è§†åŒ–ğŸ“Š

# In[12]:


rng = np.random.RandomState(0)
Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)
ynew = model.predict(Xnew)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
lim = plt.axis()
plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)
plt.axis(lim)


# ğŸ’¡å¯ä»¥çœ‹åˆ°ï¼Œé«˜æ–¯æœ´ç´ è´å¶æ–¯çš„æœ‰ä¸€æ¡åˆ†ç•Œçº¿ï¼Œè¿™æ¡çº¿å¸¸å¸¸æ˜¯äºŒæ¬¡æ–¹æ›²çº¿

# #### ğŸ”‘å¯è§†åŒ–é«˜æ–¯åˆ†å¸ƒ
# ç»“è®ºï¼šé«˜æ–¯åˆ†å¸ƒè¡°å‡æä¸ºè¿…é€Ÿã€‚

# In[9]:


# è·å–æ¯ä¸ªç±»åˆ«çš„å‡å€¼å’Œæ–¹å·®
means = model.theta_
covariances = model.var_

# è®¾å®šç½‘æ ¼èŒƒå›´
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))

# åˆ›å»ºå›¾åƒ
plt.figure(figsize=(8, 6))

# ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„é«˜æ–¯åˆ†å¸ƒç­‰é«˜çº¿
for i in range(len(means)):
    # åˆ›å»ºä¸€ä¸ªäºŒç»´æ­£æ€åˆ†å¸ƒ
    rv = multivariate_normal(mean=means[i], cov=np.diag(
        covariances[i]))  # np.diagåˆ›å»ºå¯¹è§’çº¿çŸ©é˜µ
    # è®¡ç®—æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å€¼
    Z = rv.pdf(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    print(Z.max())
    # ç»˜åˆ¶ç­‰é«˜çº¿
    contours = plt.contour(xx, yy, Z, levels=5, cmap="Blues", alpha=0.6)
    # ä¸ºç­‰é«˜çº¿æ·»åŠ æ ‡ç­¾
    plt.clabel(contours, inline=True, fontsize=8, fmt="%.2f")
# ç»˜åˆ¶æ•°æ®ç‚¹
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap='coolwarm')
plt.title('Gaussian Naive Bayes - Gaussian Distribution Contours')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar()


#

# ### 5.5.3 å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯

# #### 1. ç¤ºä¾‹ï¼šæ–°é—»åˆ†ç±»

# In[17]:


data = fetch_20newsgroups()
data.target_names


# In[18]:


categories = data.target_names
train = fetch_20newsgroups(subset='train', categories=categories)
test = fetch_20newsgroups(subset='test', categories=categories)
print(train.data[5])


# **æ¨¡å‹å­¦ä¹ **

# In[19]:


model = make_pipeline(TfidfVectorizer(), MultinomialNB())


# In[20]:


model.fit(train.data, train.target)
labels = model.predict(test.data)


# In[21]:


mat = confusion_matrix(test.target, labels)


# In[22]:


sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
            xticklabels=train.target_names, yticklabels=train.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label')


# In[23]:


def predict_category(s, train=train, model=model):
    pred = model.predict([s])
    return train.target_names[pred[0]]


predict_category('sending a payload to the ISS')


# #### 2. ç¤ºä¾‹ï¼š ä»Šæ—¥å¤´æ¡æ–°é—»åˆ†ç±»

# In[128]:


f = open('toutiao_cat_data.txt', encoding='utf8')
data = f.readlines()
data = [line.split('_!_') for line in data]
data = pd.DataFrame(
    data, columns=['æ–°é—»ID', 'åˆ†ç±»code', 'åˆ†ç±»åç§°', 'æ–°é—»å­—ç¬¦ä¸²ï¼ˆä»…å«æ ‡é¢˜ï¼‰', 'æ–°é—»å…³é”®è¯'])
data.info()


# In[129]:


data.describe()


# In[130]:


data.head()


# In[131]:


print(data[data['æ–°é—»ID'] == '6554616677084955139'])


# **1. æ•°æ®æ¸…ç†**
# - åªåŒ¹é…ï¼Œä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦å’Œæ•°å­—
# - ä¸­è‹±æ–‡ç¬¦åˆç»Ÿä¸€

# In[132]:


data['æ–°é—»'] = data['æ–°é—»å…³é”®è¯'] + '' + data['æ–°é—»å­—ç¬¦ä¸²ï¼ˆä»…å«æ ‡é¢˜ï¼‰']


# In[133]:


symbol_map = str.maketrans({
    '!': 'ï¼',
    '?': 'ï¼Ÿ',
    ',': 'ï¼Œ',
    '.': 'ã€‚',
    ':': 'ï¼š',
    ';': 'ï¼›',
    "'": 'â€™',
    '"': 'â€œ',
    '(': 'ï¼ˆ',
    ')': 'ï¼‰',
    '-': 'â€”â€”',
    '_': 'ï¼¿'
    # æ·»åŠ å…¶ä»–ç¬¦å·çš„æ˜ å°„
})
data['æ–°é—»'] = data['æ–°é—»'].str.translate(symbol_map)  # translateé›†ä½“æ˜ å°„æ›¿æ¢


# In[134]:


data['æ–°é—»'].tail()


# In[135]:


data['æ–°é—»'] = data['æ–°é—»'].str.replace('\n', '')
data['æ–°é—»'].head()


# In[136]:


data['æ–°é—»é•¿åº¦'] = data['æ–°é—»'].apply(len)
longest_text = data.loc[data['æ–°é—»é•¿åº¦'].idxmax()]  # æ‰¾åˆ°æœ€é•¿çš„ä¸€è¡Œ
longest_text


# In[137]:


X_news = data['æ–°é—»']  # å‘é‡åŒ–åªèƒ½å¤„ç†ä¸€åˆ—
Y_news = data['åˆ†ç±»åç§°']
print(X_news.shape, Y_news.shape)


# **2.åˆ†å‰²æ•°æ®é›†**

# In[138]:


Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_news, Y_news, random_state=1)
print(Xtrain.shape, Ytrain.shape, Xtest.shape)


# **ğŸš€ TFä½¿ç”¨ç©ºæ ¼å’Œï¼Œåˆ†è¯ï¼Œ å¯¹ä¸­æ–‡æ²¡æœ‰æ•ˆæœã€‚ éœ€è¦è‡ªå®šä¹‰å…ˆå¯¹ä¸­æ–‡åˆ†è¯ã€‚**

# In[139]:


def jieba_tokenizer(txt):
    return jieba.lcut(txt)


result = jieba_tokenizer('å‘é…µåºŠçš„å«æ–™ç§ç±»æœ‰å“ªäº›ï¼Ÿå“ªç§æ›´å¥½ï¼Ÿ')
print(result)


# **3. æ¨¡å‹å­¦ä¹ **

# In[149]:


model = make_pipeline(TfidfVectorizer(
    tokenizer=jieba_tokenizer, token_pattern=None), MultinomialNB(alpha=1))


# In[150]:


model.fit(Xtrain, Ytrain)


# In[151]:


vectorizer = model.named_steps['tfidfvectorizer']
features = vectorizer.get_feature_names_out()
print(type(features))  # æŸ¥çœ‹åˆ†è¯ç‰¹å¾
print(len(features))
print(features[:30])


# **4. æ¨¡å‹é¢„æµ‹**

# In[152]:


predicated = model.predict(Xtest)
predicated_sobj = pd.Series(predicated, index=Ytest.index)
mat_df = pd.DataFrame({'é¢„æµ‹åç§°': predicated_sobj, 'åˆ†ç±»åç§°': Ytest})


# **5. å‡†ç¡®ç‡**

# In[153]:


accuracy_score(predicated, Ytest)


# **6. æ··æ·†çŸ©é˜µ**

# In[154]:


labels = np.unique(Y_news)
mat = confusion_matrix(Ytest, predicated, labels=labels)
plt.figure(figsize=(10, 8))  # è®¾ç½®å®½åº¦ä¸º10ï¼Œé«˜åº¦ä¸º8
sns.heatmap(mat.T, square=True, annot=True, fmt='d',
            xticklabels=labels, yticklabels=labels, cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label')


# In[148]:


print("Precision:", precision_score(Ytest, predicated, average='weighted'))
print("Recall:", recall_score(Ytest, predicated, average='weighted'))
print("F1 Score:", f1_score(Ytest, predicated, average='weighted'))


# ç»“è®ºï¼šæ€»æ˜¯è¯¯åˆ†ç±»åˆ°news_tech

# In[146]:


category_counts = data['åˆ†ç±»åç§°'].value_counts()
print(category_counts)


# In[ ]:


# ## 5.6ã€€ä¸“é¢˜ï¼šçº¿æ€§å›å½’
#

# > å¦‚æœè¯´æœ´ç´ è´å¶æ–¯æ˜¯è§£å†³åˆ†ç±»ä»»åŠ¡çš„å¥½èµ·ç‚¹ï¼Œé‚£ä¹ˆçº¿æ€§å›å½’æ¨¡å‹å°±æ˜¯è§£å†³å›å½’ä»»åŠ¡çš„å¥½èµ·ç‚¹ã€‚æ‹Ÿåˆé€Ÿåº¦éå¸¸å¿«ï¼Œè€Œ
# ä¸”å¾ˆå®¹æ˜“è§£é‡Šã€‚

# In[3]:


sns.set()


# ### 5.6.1ã€€ç®€å•çº¿æ€§å›å½’

# In[11]:


rng = np.random.RandomState(0)
x = 10 * rng.rand(50)
y = 2 * x - 5 + rng.randn(50)
plt.scatter(x, y)


# In[15]:


model = LinearRegression(fit_intercept=True)
model.fit(x[:, np.newaxis], y)

xfit = np.linspace(0, 10, 1000)
yfit = model.predict(xfit[:, np.newaxis])
plt.scatter(x, y)
plt.plot(xfit, yfit)


# In[17]:


print("Model slope:    ", model.coef_[0])  # æ¥è¿‘2
print("Model intercept:", model.intercept_)  # æ¥è¿‘-5


# ### 5.6.2ã€€åŸºå‡½æ•°å›å½’
# åŸºå‡½æ•°å°†å˜é‡çš„çº¿æ€§å›å½’è½¬ä¸ºéçº¿æ€§å›å½’ã€‚ ä»ç„¶æ˜¯çº¿æ€§æ¨¡å‹ã€‚ ï¼ˆ 5.3 èŠ‚å’Œ 5.4 èŠ‚ï¼‰

# 1. å¤šé¡¹å¼åŸºå‡½æ•°

# è½¬å˜ä¸ºåªæ˜¯é«˜æ¬¡å›å½’

# In[18]:


x = np.array([2, 3, 4])
poly = PolynomialFeatures(3, include_bias=False)  # è½¬æ¢å™¨å°†ä¸€ç»´æ•°ç»„è½¬æ¢ä¸º3ç»´
poly.fit_transform(x[:, None])


# In[19]:


poly_model = make_pipeline(PolynomialFeatures(7),
                           LinearRegression())
rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = np.sin(x) + 0.1 * rng.randn(50)

poly_model.fit(x[:, np.newaxis], y)
yfit = poly_model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit)


# 2. é«˜æ–¯åŸºå‡½æ•°

# ### 5.6.3ã€€æ­£åˆ™åŒ–

# In[ ]:


# In[ ]:


# In[ ]:


# ### 5.6.4ã€€æ¡ˆä¾‹ï¼šé¢„æµ‹è‡ªè¡Œè½¦æµé‡
# åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§ å›å½’æ¨¡å‹æ¥æ¢ç´¢ä¸è‡ªè¡Œè½¦æ•°é‡ç›¸å…³çš„å¤©æ°”å’Œå…¶ä»–å› ç´ ï¼Œä»è€Œè¯„ä¼°ä»»æ„ä¸€ç§å› ç´ å¯¹éª‘è½¦äººæ•° çš„å½±å“ã€‚

# In[ ]:


# In[ ]:


# In[ ]:


# ## 5.7ã€€ä¸“é¢˜ï¼šæ”¯æŒå‘é‡æœº
# å¯åˆ†ç±»ï¼Œå¯å›å½’

# In[ ]:


# In[2]:


sns.set()


# In[3]:


X, y = make_blobs(n_samples=50, centers=2,
                  random_state=0, cluster_std=0.60)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')


# æœ‰å¤šæ¡çº¿å¯ä»¥åˆ†å‰²ï¼Œé€‰æ‹©ä¸€ä¸ªæœ€å¥½çš„ã€‚

# In[4]:


xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)

for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:
    plt.plot(xfit, m * xfit + b, '-k')

plt.xlim(-1, 3.5)


# ### 5.7.2ã€€æ”¯æŒå‘é‡æœºï¼šè¾¹ç•Œæœ€å¤§åŒ–

# In[5]:


xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
    yfit = m * xfit + b
    plt.plot(xfit, yfit, '-k')
    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA',
                     alpha=0.4)

plt.xlim(-1, 3.5)


# 1. æ‹Ÿåˆæ”¯æŒå‘é‡æœº

# In[6]:


model = SVC(kernel='linear', C=1E10)
model.fit(X, y)


# In[7]:


def plot_svc_decision_function(model, ax=None, plot_support=True):
    """ç”»äºŒç»´SVCçš„å†³ç­–å‡½æ•°"""
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    # åˆ›å»ºè¯„ä¼°æ¨¡å‹çš„ç½‘æ ¼
    x = np.linspace(xlim[0], xlim[1], 30)
    y = np.linspace(ylim[0], ylim[1], 30)
    Y, X = np.meshgrid(y, x)  # xåæ ‡é›†(n,1)ï¼Œ yåæ ‡é›†(n,1)
    xy = np.vstack([X.ravel(), Y.ravel()]).T  # (n, 2) çš„äºŒç»´æ•°ç»„ xy
    P = model.decision_function(xy).reshape(X.shape)  # åˆ°å†³ç­–è¾¹ç•Œçš„è·ç¦»

    # ç”»å†³ç­–è¾¹ç•Œå’Œè¾¹ç•Œ
    ax.contour(X, Y, P, colors='k',   levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])
    # ç”»æ”¯æŒå‘é‡
    if plot_support:
        ax.scatter(model.support_vectors_[:, 0],
                   model.support_vectors_[:, 1],
                   s=300, linewidth=1, facecolors='none')
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)


# In[8]:


plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(model)


# In[9]:


model.support_vectors_


# In[12]:


def plot_svm(N, ax=None):
    X, y = make_blobs(n_samples=N, centers=2,
                      random_state=0, cluster_std=0.60)
    model = SVC(kernel='linear', C=1E10)
    model.fit(X, y)

    ax = ax or plt.gca()
    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
    ax.set_xlim(-1, 4)
    ax.set_ylim(-1, 6)
    plot_svc_decision_function(model, ax)


fig, axes = plt.subplots(1, 2, figsize=(16, 6))
for axi, N in zip(axes, [60, 120]):
    plot_svm(N, axi)
    axi.set_title(f'N = {N}')
plt.show()


# In[11]:


interact(plot_svm, N=[10, 200], ax=fixed(None))


# 2. è¶…è¶Šçº¿æ€§è¾¹ç•Œï¼šæ ¸å‡½æ•°SVMæ¨¡å‹
#
# éçº¿æ€§å¯åˆ†åŸæ•°æ® é€šè¿‡ æ ¸å‡½æ•° çº¿æ€§å¯åˆ†

# In[ ]:


# In[14]:


X, y = make_circles(100, factor=.1, noise=.1)

clf = SVC(kernel='linear').fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf, plot_support=False)


# In[15]:


r = np.exp(-(X ** 2).sum(1))


# In[ ]:


# In[18]:


def plot_3D(elev=30, azim=30, X=X, y=y):
    ax = plt.subplot(projection='3d')
    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')
    ax.view_init(elev=elev, azim=azim)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_zlabel('r')


plot_3D()


# **ä½†é€šå¸¸å¾ˆéš¾é€‰æ‹©åŸºå‡½æ•°ï¼Œè€Œä¸æ˜¯åƒè¿™ä¹ˆç®€å•**ã€‚
# SVCé€šè¿‡æ ¸å‚æ•°è®¾ç½®

# In[21]:


clf = SVC(kernel='rbf', C=1E6)
clf.fit(X, y)


plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf)
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
            s=200, lw=2, facecolors='none', edgecolors='red')  # âœ… è¿™æ ·ä¸€èˆ¬èƒ½æ­£ç¡®æ¸²æŸ“åœˆåœˆ


# **3. SVMä¼˜åŒ–ï¼šè½¯åŒ–è¾¹ç•Œ**
# å¦‚æœä½ çš„æ•°æ®æœ‰ä¸€äº›é‡å è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ

# In[24]:


X, y = make_blobs(n_samples=100, centers=2,
                  random_state=0, cluster_std=1.2)  # cluster_std æ ‡å‡†å·®ï¼Œæ§åˆ¶é›†ç¾¤çš„åˆ†æ•£ç¨‹åº¦
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')


# In[26]:


X, y = make_blobs(n_samples=100, centers=2,
                  random_state=0, cluster_std=0.8)

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for axi, C in zip(ax, [10.0, 0.1]):
    model = SVC(kernel='linear', C=C).fit(X, y)  # Cå‚æ•°æ§åˆ¶èƒ½æœ‰å¤šå°‘ç‚¹åœ¨è¾¹ç•Œå†…
    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
    plot_svc_decision_function(model, axi)
    axi.scatter(model.support_vectors_[:, 0],
                model.support_vectors_[:, 1],
                s=300, lw=1, facecolors='none',  edgecolors='red')
    axi.set_title('C = {0:.1f}'.format(C), size=14)


# ### 5.7.3ã€€æ¡ˆä¾‹ï¼šäººè„¸è¯†åˆ«
# Wild æ•°æ®é›†ä¸­å¸¦æ ‡è®°çš„äººè„¸
#

# In[1]:


faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names)
print(faces.images.shape)


# In[4]:


# In[8]:


fig, ax = plt.subplots(3, 5)
for i, axi in enumerate(ax.flat):
    axi.imshow(faces.images[i], cmap='bone')
    axi.set(xticks=[], yticks=[],
            xlabel=faces.target_names[faces.target[i]])


# æ¯ä¸ªå›¾åƒåŒ…å« [62Ã—47]ã€æ¥è¿‘ 3000 åƒç´ ã€‚è™½ç„¶å¯ä»¥ç®€å•åœ°å°†æ¯ä¸ªåƒç´ ä½œä¸ºä¸€ä¸ªç‰¹å¾ã€‚ä½†é€šå¸¸ä½¿ç”¨é™ç»´ï¼Œä¸‹é¢ç”¨PCAæå–150ä¸ª

# In[9]:


pca = PCA(n_components=150, whiten=True,
          random_state=42, svd_solver='randomized')
svc = SVC(kernel='rbf', class_weight='balanced')
model = make_pipeline(pca, svc)


# In[13]:


Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,
                                                random_state=42)


# **ç”¨ç½‘æ ¼æœç´¢äº¤å‰æ£€éªŒæ¥å¯»æ‰¾æœ€ä¼˜å‚æ•°ç»„åˆã€‚**
# å‚æ•° Cï¼ˆæ§åˆ¶è¾¹ç•Œçº¿çš„ç¡¬ åº¦ï¼‰å’Œå‚æ•° gammaï¼ˆæ§åˆ¶å¾„å‘åŸºå‡½æ•°æ ¸çš„å¤§å°ï¼‰ï¼Œ

# In[15]:


param_grid = {'svc__C': [1, 5, 10, 50],
              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}
grid = GridSearchCV(model, param_grid)

get_ipython().run_line_magic('time', 'grid.fit(Xtrain, ytrain)')
print(grid.best_params_)


# In[16]:


model = grid.best_estimator_
yfit = model.predict(Xtest)


# In[18]:


fig, ax = plt.subplots(4, 6)
for i, axi in enumerate(ax.flat):
    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')
    axi.set(xticks=[], yticks=[])
    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],
                   color='black' if yfit[i] == ytest[i] else 'red')
fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)


# åˆ†ç±»æŠ¥å‘Š

# In[19]:


print(classification_report(ytest, yfit,
                            target_names=faces.target_names))


# ç¤ºä¾‹ï¼š èšç±»

# In[134]:


Xtrain.toarray()


# In[137]:


model = make_pipeline(CountVectorizer(), GaussianMixture(
    n_components=len(labels), covariance_type='full'))


# In[138]:


model.fit(Xtrain)


# https://cloud.tencent.com/developer/ask/sof/111732133

#
